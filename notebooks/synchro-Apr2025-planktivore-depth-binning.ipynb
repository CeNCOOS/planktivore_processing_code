{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc2c307d",
   "metadata": {},
   "source": [
    "# Binning LRAUV Data\n",
    "\n",
    "Classification output (inference) is matched to the LRAUV data at the `inference_out/lrauv_april2025_with_class.parquet` file. Thata file was generated in the `synchro-Apr2025-planktivore-occurance-high-mag-bytime.ipynb` notebook.\n",
    "\n",
    "- This data is loaded into a `pandas dataframe` and run through the binning function: `bin_casts_avg_with_time_updown`.\n",
    "    - Parameters for binning can be set here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f255a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "import gsw\n",
    "fig_dir=Path(\"c:/planktivore/figures/\")\n",
    "#fig_dir = Path(\"../../figures\")\n",
    "p=Path(\"c:/planktivore/\")\n",
    "#p = Path(\"../../inference_out/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19961a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_casts_avg_with_time_updown(\n",
    "    df: pd.DataFrame,\n",
    "    depth_col: str = \"depth\",\n",
    "    cast_col: str = \"cast_id\",\n",
    "    time_col: Optional[str] = None,    # if None, use datetime index\n",
    "    bin_size: float = 1.0,\n",
    "    phase_labels: Tuple[str, str] = (\"down\", \"up\"),\n",
    "    smooth_window: int = 15,            # samples (rolling median) to stabilize direction\n",
    "    closed: str = \"right\",             # Interval closure for pd.cut\n",
    "    per_phase_edges: bool = False,     # if True, compute bin edges separately for up/down\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each cast, split trajectory into down/up phases, bin by depth, and compute:\n",
    "      - numeric means per bin,\n",
    "      - start_time, end_time (first/last timestamps observed in the bin),\n",
    "      - elapsed_seconds (sum of dt while consecutive samples stay in the same bin),\n",
    "      - rep_timestamp (timestamp of sample closest to bin center).\n",
    "\n",
    "    Returns a tidy DataFrame indexed by [cast_id, phase, depth_bin_center].\n",
    "    \"\"\"\n",
    "    def custom_sum_with_min_counter(series):\n",
    "        return series.sum(min_count=1)\n",
    "\n",
    "    # ---- Prep time column ----\n",
    "    work = df.copy()\n",
    "    if time_col is None:\n",
    "        if not np.issubdtype(work.index.dtype, np.datetime64):\n",
    "            raise ValueError(\"time_col is None but index is not datetime-like.\")\n",
    "        work[\"__time__\"] = work.index\n",
    "        time_col = \"__time__\"\n",
    "    else:\n",
    "        if not np.issubdtype(work[time_col].dtype, np.datetime64):\n",
    "            work[time_col] = pd.to_datetime(work[time_col])\n",
    "\n",
    "    # ---- Basic checks ----\n",
    "    for c in (cast_col, depth_col, time_col):\n",
    "        if c not in work.columns:\n",
    "            raise ValueError(f\"Missing required column: {c}\")\n",
    "\n",
    "    # The code below removes NaNs but we don't want them to become zeros.  We want NaNs\n",
    "    ## ---- Clean & sort ----\n",
    "    #work = (\n",
    "    #    work.dropna(subset=[cast_col, depth_col, time_col])\n",
    "    #        .sort_values([cast_col, time_col])\n",
    "    #)\n",
    "\n",
    "    out_frames = []\n",
    "\n",
    "    # ---- Process each cast ----\n",
    "    for cid, g_cast in work.groupby(cast_col, sort=True):\n",
    "        # Ensure numeric columns are float64 for proper NaN handling\n",
    "        numerical_cols=['fix_latitude', 'fix_longitude', 'salinity', 'temperature',\n",
    "       'mass_concentration_of_oxygen_in_sea_water',\n",
    "       'bin_mean_sea_water_salinity', 'bin_median_sea_water_salinity',\n",
    "       'bin_mean_sea_water_temperature', 'bin_median_sea_water_temperature',\n",
    "       'PAR', 'chlorophyll', 'chl', 'bbp470', 'bbp650',\n",
    "       'volumescatcoeff117deg470nm', 'volumescatcoeff117deg650nm',\n",
    "       'particulatebackscatteringcoeff470nm',\n",
    "       'particulatebackscatteringcoeff650nm',\n",
    "       'fix_residual_percent_distance_traveled_DeadReckonUsingMultipleVelocitySources',\n",
    "       'latitude', 'longitude', 'depth', 'cast_id',\n",
    "       'cast_id_unique', 'distance_km', 'rois_count', 'rois_norm', 'Akashiwo',\n",
    "       'Amphidinium_Oxyphysis', 'Ceratium', 'Chaetoceros', 'Ciliate',\n",
    "       'Cylindrotheca', 'Detonula_Cerataulina_Lauderia', 'Detritus',\n",
    "       'Dictyocha', 'Dinoflagellate', 'Eucampia', 'Guinardia_Dactyliosolen',\n",
    "       'Gyrodinium', 'Medium_pennate', 'Mesodinium', 'Mixed_diatom_chain',\n",
    "       'Nano_plankton', 'Polykrikos', 'Prorocentrum', 'Pseudo-nitzschia',\n",
    "       'Strombidium', 'Thalassionema', 'Thalassiosira', 'Tiarina', 'Truncated',\n",
    "       'unknown_flagellate']\n",
    "        g_cast[numerical_cals]=g_cast[numerical_cols].astype('float64')\n",
    "        # end of new code\n",
    "\n",
    "        g_cast = g_cast.sort_values(time_col).copy()\n",
    "\n",
    "        # Direction: positive depth change => down; negative => up\n",
    "        ddepth = g_cast[depth_col].diff()\n",
    "        if smooth_window and smooth_window > 1:\n",
    "            ddepth = ddepth.rolling(smooth_window, center=True, min_periods=1).median()\n",
    "\n",
    "        sign = np.sign(ddepth).replace({0: np.nan})\n",
    "        sign = sign.ffill().bfill()\n",
    "        phase_map = {1.0: phase_labels[0], -1.0: phase_labels[1]}\n",
    "        g_cast[\"phase\"] = sign.map(phase_map)\n",
    "\n",
    "        if g_cast[\"phase\"].isna().all():\n",
    "        #    # Degenerate (flat) cast â€” skip\n",
    "            continue\n",
    "\n",
    "        # Bin edges (shared across phases for this cast unless per_phase_edges=True)\n",
    "        def make_edges(g):\n",
    "            dmin = float(np.floor(g[depth_col].min()))\n",
    "            dmax = float(np.ceil(g[depth_col].max()))\n",
    "            edges = np.arange(dmin, dmax + bin_size, bin_size)\n",
    "            return edges if edges.size >= 2 else None\n",
    "\n",
    "        shared_edges = make_edges(g_cast) if not per_phase_edges else None\n",
    "        if not per_phase_edges and shared_edges is None:\n",
    "            continue\n",
    "\n",
    "        # ---- Process each phase ----\n",
    "        for ph, gp in g_cast.groupby(\"phase\", sort=True):\n",
    "            if gp.empty:\n",
    "                continue\n",
    "\n",
    "            edges = make_edges(gp) if per_phase_edges else shared_edges\n",
    "            if edges is None:\n",
    "                continue\n",
    "\n",
    "            ivals = pd.IntervalIndex.from_breaks(edges, closed=closed)\n",
    "\n",
    "            gg = gp.sort_values(time_col).copy()\n",
    "            gg[\"_bin\"] = pd.cut(gg[depth_col], bins=ivals)\n",
    "            #\n",
    "            # New aggregation logic\n",
    "            # add code for aggregation_columns\n",
    "            aggregation_columns = [col for col in gg.columns if col not in [cast_col, time_col, depth_col, \"phase\", \"_bin\"]]\n",
    "\n",
    "            keywords=['fix_latitude', 'fix_longitude', 'salinity', 'temperature',\n",
    "            'mass_concentration_of_oxygen_in_sea_water',\n",
    "            'bin_mean_sea_water_salinity', 'bin_median_sea_water_salinity',\n",
    "            'bin_mean_sea_water_temperature', 'bin_median_sea_water_temperature',\n",
    "            'PAR', 'chlorophyll', 'chl', 'bbp470', 'bbp650',\n",
    "            'volumescatcoeff117deg470nm', 'volumescatcoeff117deg650nm',\n",
    "            'particulatebackscatteringcoeff470nm',\n",
    "            'particulatebackscatteringcoeff650nm',\n",
    "            'fix_residual_percent_distance_traveled_DeadReckonUsingMultipleVelocitySources',\n",
    "            'latitude', 'longitude', 'depth', 'phase', 'cast_id', 'turning_point',\n",
    "            'cast_id_unique', 'distance_km']\n",
    "            # do we need other keywords?  Do we want to add rois_count and the species rates?\n",
    "            \n",
    "            agg_dict = {}\n",
    "            for col in aggregation_columns:\n",
    "                col_lower=col.lower()\n",
    "                if any(keyword in col_lower for keyword in keywords):\n",
    "                    agg_dict[col] = 'mean'\n",
    "                else:\n",
    "                    agg_dict[col] = custom_sum_with_min_counter\n",
    "                    #agg_dict[col] = 'sum'\n",
    "\n",
    "            # Numeric means per observed bin\n",
    "            # why do we use a mean here and not the sum?\n",
    "            agg= gg.groupby(\"_bin\", observed=True).agg(agg_dict)\n",
    "            #\n",
    "            # end of new code\n",
    "            #\n",
    "            # old code:\n",
    "            #agg = gg.groupby(\"_bin\", observed=True).mean(numeric_only=True)\n",
    "            if agg.empty:\n",
    "                continue\n",
    "\n",
    "            # Bin center\n",
    "            bin_center_col = f\"{depth_col}_bin_center\"\n",
    "            agg[bin_center_col] = [iv.mid for iv in agg.index]\n",
    "            agg[cast_col] = cid\n",
    "            agg[\"phase\"] = ph\n",
    "\n",
    "            # Representative timestamp: nearest to bin center within phase subset\n",
    "            rep_times = []\n",
    "            for iv in agg.index:\n",
    "                sub = gg.loc[gg[\"_bin\"] == iv]\n",
    "                if sub.empty:\n",
    "                    rep_times.append(pd.NaT)\n",
    "                    continue\n",
    "                mid = iv.mid\n",
    "                idx = (sub[depth_col] - mid).abs().idxmin()\n",
    "                rep_times.append(gg.loc[idx, time_col])\n",
    "            agg[\"rep_timestamp\"] = pd.to_datetime(rep_times)\n",
    "\n",
    "            # --- Time-in-bin: sum dt where next sample remains in the same bin ---\n",
    "            gs = gg.copy()\n",
    "            gs[\"__next_time__\"] = gs[time_col].shift(-1)\n",
    "            gs[\"__next_bin__\"] = gs[\"_bin\"].shift(-1)\n",
    "            # original line:\n",
    "            mask = gs[\"_bin\"].notna() & (gs[\"_bin\"] == gs[\"__next_bin__\"])\n",
    "            #mask = gs[\"_bin\"] == gs[\"__next_bin__\"]\n",
    "            dt = (gs.loc[mask, \"__next_time__\"] - gs.loc[mask, time_col]).dt.total_seconds()\n",
    "            elapsed = dt.groupby(gs.loc[mask, \"_bin\"], observed=True).sum(min_count=1)\n",
    "            \n",
    "            elapsed = elapsed.reindex(agg.index)  # align to bins present in agg\n",
    "\n",
    "            # Start/end times per bin (first/last timestamps observed in that bin)\n",
    "            ts_first = gg.groupby(\"_bin\", observed=True)[time_col].first().reindex(agg.index)\n",
    "            ts_last  = gg.groupby(\"_bin\", observed=True)[time_col].last().reindex(agg.index)\n",
    "\n",
    "            agg[\"start_time\"] = pd.to_datetime(ts_first.values)\n",
    "            agg[\"end_time\"] = pd.to_datetime(ts_last.values)\n",
    "            agg[\"elapsed_seconds\"]=elapsed.astype(float)\n",
    "            # Supposedly this line gets rid of all NaNs but we need to keep them and true zeros\n",
    "            #agg[\"elapsed_seconds\"] = elapsed.fillna(0.0).astype(float)\n",
    "\n",
    "            out_frames.append(agg.reset_index(drop=True))\n",
    "\n",
    "    if not out_frames:\n",
    "        cols = [cast_col, \"phase\", f\"{depth_col}_bin_center\", \"rep_timestamp\", \"start_time\", \"end_time\", \"elapsed_seconds\"]\n",
    "        return pd.DataFrame(columns=cols).set_index([cast_col, \"phase\", f\"{depth_col}_bin_center\"])\n",
    "\n",
    "    result = (\n",
    "        pd.concat(out_frames, ignore_index=True)\n",
    "          .set_index([cast_col, \"phase\", f\"{depth_col}_bin_center\"])\n",
    "          .sort_index()\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de0e4a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_parquet(p / \"lrauv_april2025_with_class.parquet\")\n",
    "#df = pd.read_parquet(p / \"lrauv_april2025_with_class_20251009model_filtered.parquet\")\n",
    "df = pd.read_parquet(p / \"lrauv_april2025_with_class_20251009model_filtered_addNaN.parquet\")\n",
    "#df=pd.read_parquet(p / \"inference_results_mbari-ptvr-vits-b8-20250826_synchro_202504.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27093c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time\n",
       "2025-04-14 21:00:16    <NA>\n",
       "2025-04-14 21:00:18    <NA>\n",
       "2025-04-14 21:00:20    <NA>\n",
       "2025-04-14 21:00:22    <NA>\n",
       "2025-04-14 21:00:24    <NA>\n",
       "                       ... \n",
       "2025-04-18 15:31:52    <NA>\n",
       "2025-04-18 15:31:54    <NA>\n",
       "2025-04-18 15:31:56    <NA>\n",
       "2025-04-18 15:31:58    <NA>\n",
       "2025-04-18 15:32:00    <NA>\n",
       "Name: rois_count, Length: 147085, dtype: Int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rois_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c97ac2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fix_latitude', 'fix_longitude', 'salinity', 'temperature',\n",
       "       'mass_concentration_of_oxygen_in_sea_water',\n",
       "       'bin_mean_sea_water_salinity', 'bin_median_sea_water_salinity',\n",
       "       'bin_mean_sea_water_temperature', 'bin_median_sea_water_temperature',\n",
       "       'PAR', 'chlorophyll', 'chl', 'bbp470', 'bbp650',\n",
       "       'volumescatcoeff117deg470nm', 'volumescatcoeff117deg650nm',\n",
       "       'particulatebackscatteringcoeff470nm',\n",
       "       'particulatebackscatteringcoeff650nm',\n",
       "       'fix_residual_percent_distance_traveled_DeadReckonUsingMultipleVelocitySources',\n",
       "       'latitude', 'longitude', 'depth', 'phase', 'cast_id', 'turning_point',\n",
       "       'cast_id_unique', 'distance_km', 'rois_count', 'rois_norm', 'Akashiwo',\n",
       "       'Amphidinium_Oxyphysis', 'Ceratium', 'Chaetoceros', 'Ciliate',\n",
       "       'Cylindrotheca', 'Detonula_Cerataulina_Lauderia', 'Detritus',\n",
       "       'Dictyocha', 'Dinoflagellate', 'Eucampia', 'Guinardia_Dactyliosolen',\n",
       "       'Gyrodinium', 'Medium_pennate', 'Mesodinium', 'Mixed_diatom_chain',\n",
       "       'Nano_plankton', 'Polykrikos', 'Prorocentrum', 'Pseudo-nitzschia',\n",
       "       'Strombidium', 'Thalassionema', 'Thalassiosira', 'Tiarina', 'Truncated',\n",
       "       'unknown_flagellate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ee28baae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll=df['cast_id']==1\n",
    "cast_col='cast_id'\n",
    "depth_col='depth'\n",
    "time_col=None\n",
    "bin_size=5\n",
    "smooth_window=5\n",
    "phase_labels=[\"down\", \"up\"]\n",
    "closed='right'\n",
    "per_phase_edges: bool = False\n",
    "work = df.copy()\n",
    "work=work[ll]\n",
    "def custom_sum_with_min_counter(series):\n",
    "    return series.sum(min_count=1)\n",
    "if time_col is None:\n",
    "    if not np.issubdtype(work.index.dtype, np.datetime64):\n",
    "        raise ValueError(\"time_col is None but index is not datetime-like.\")\n",
    "    work[\"__time__\"] = work.index\n",
    "    time_col = \"__time__\"\n",
    "else:\n",
    "    if not np.issubdtype(work[time_col].dtype, np.datetime64):\n",
    "        work[time_col] = pd.to_datetime(work[time_col])\n",
    "\n",
    "    # ---- Basic checks ----\n",
    "for c in (cast_col, depth_col, time_col):\n",
    "    if c not in work.columns:\n",
    "        raise ValueError(f\"Missing required column: {c}\")\n",
    "    out_frames = []\n",
    "\n",
    "# ---- Process each cast ----\n",
    "for cid, g_cast in work.groupby(cast_col, sort=True):\n",
    "    numerical_cols=['fix_latitude', 'fix_longitude', 'salinity', 'temperature',\n",
    "       'mass_concentration_of_oxygen_in_sea_water',\n",
    "       'bin_mean_sea_water_salinity', 'bin_median_sea_water_salinity',\n",
    "       'bin_mean_sea_water_temperature', 'bin_median_sea_water_temperature',\n",
    "       'PAR', 'chlorophyll', 'chl', 'bbp470', 'bbp650',\n",
    "       'volumescatcoeff117deg470nm', 'volumescatcoeff117deg650nm',\n",
    "       'particulatebackscatteringcoeff470nm',\n",
    "       'particulatebackscatteringcoeff650nm',\n",
    "       'fix_residual_percent_distance_traveled_DeadReckonUsingMultipleVelocitySources',\n",
    "       'latitude', 'longitude', 'depth', 'cast_id',\n",
    "       'cast_id_unique', 'distance_km', 'rois_count', 'rois_norm', 'Akashiwo',\n",
    "       'Amphidinium_Oxyphysis', 'Ceratium', 'Chaetoceros', 'Ciliate',\n",
    "       'Cylindrotheca', 'Detonula_Cerataulina_Lauderia', 'Detritus',\n",
    "       'Dictyocha', 'Dinoflagellate', 'Eucampia', 'Guinardia_Dactyliosolen',\n",
    "       'Gyrodinium', 'Medium_pennate', 'Mesodinium', 'Mixed_diatom_chain',\n",
    "       'Nano_plankton', 'Polykrikos', 'Prorocentrum', 'Pseudo-nitzschia',\n",
    "       'Strombidium', 'Thalassionema', 'Thalassiosira', 'Tiarina', 'Truncated',\n",
    "       'unknown_flagellate']\n",
    "    g_cast[numerical_cals]=g_cast[numerical_cols].astype('float64')\n",
    "    g_cast = g_cast.sort_values(time_col).copy()\n",
    "    # Direction: positive depth change => down; negative => up\n",
    "    ddepth = g_cast[depth_col].diff()\n",
    "    if smooth_window and smooth_window > 1:\n",
    "        ddepth = ddepth.rolling(smooth_window, center=True, min_periods=1).median()\n",
    "        sign = np.sign(ddepth).replace({0: np.nan})\n",
    "        sign = sign.ffill().bfill()\n",
    "        phase_map = {1.0: phase_labels[0], -1.0: phase_labels[1]}\n",
    "        g_cast[\"phase\"] = sign.map(phase_map)\n",
    "    def make_edges(g):\n",
    "        dmin = float(np.floor(g[depth_col].min()))\n",
    "        dmax = float(np.ceil(g[depth_col].max()))\n",
    "        edges = np.arange(dmin, dmax + bin_size, bin_size)\n",
    "        return edges if edges.size >= 2 else None\n",
    "\n",
    "    shared_edges = make_edges(g_cast) if not per_phase_edges else None\n",
    "    if not per_phase_edges and shared_edges is None:\n",
    "        continue\n",
    "    for ph, gp in g_cast.groupby(\"phase\", sort=True):\n",
    "        if gp.empty:\n",
    "            continue\n",
    "\n",
    "        edges = make_edges(gp) if per_phase_edges else shared_edges\n",
    "        if edges is None:\n",
    "            continue\n",
    "\n",
    "        ivals = pd.IntervalIndex.from_breaks(edges, closed=closed)\n",
    "\n",
    "        gg = gp.sort_values(time_col).copy()\n",
    "        # new force data to be float\n",
    "        #gg=gg.astype('float64')\n",
    "        #\n",
    "        gg[\"_bin\"] = pd.cut(gg[depth_col], bins=ivals)\n",
    "        aggregation_columns = [col for col in gg.columns if col not in [cast_col, time_col, depth_col, \"phase\", \"_bin\"]]\n",
    "\n",
    "        keywords=['fix_latitude', 'fix_longitude', 'salinity', 'temperature',\n",
    "        'mass_concentration_of_oxygen_in_sea_water',\n",
    "        'bin_mean_sea_water_salinity', 'bin_median_sea_water_salinity',\n",
    "        'bin_mean_sea_water_temperature', 'bin_median_sea_water_temperature',\n",
    "        'PAR', 'chlorophyll', 'chl', 'bbp470', 'bbp650',\n",
    "        'volumescatcoeff117deg470nm', 'volumescatcoeff117deg650nm',\n",
    "        'particulatebackscatteringcoeff470nm',\n",
    "        'particulatebackscatteringcoeff650nm',\n",
    "        'fix_residual_percent_distance_traveled_DeadReckonUsingMultipleVelocitySources',\n",
    "        'latitude', 'longitude', 'depth', 'phase', 'cast_id', 'turning_point',\n",
    "        'cast_id_unique', 'distance_km']\n",
    "            # do we need other keywords?  Do we want to add rois_count and the species rates?\n",
    "            \n",
    "        agg_dict = {}\n",
    "        for col in aggregation_columns:\n",
    "            col_lower=col.lower()\n",
    "            if any(keyword in col_lower for keyword in keywords):\n",
    "                agg_dict[col] = 'mean'\n",
    "            else:\n",
    "                #agg_dict[col] = 'sum'\n",
    "                agg_dict[col] = custom_sum_with_min_counter\n",
    "    \n",
    "        agg= gg.groupby(\"_bin\", observed=True).agg(agg_dict)\n",
    "        #\n",
    "        # end of new code\n",
    "        #\n",
    "        # old code:\n",
    "        ##agg = gg.groupby(\"_bin\", observed=True).mean(numeric_only=True)\n",
    "        #if agg.empty:\n",
    "        #    continue\n",
    "\n",
    "        ## Bin center\n",
    "        #bin_center_col = f\"{depth_col}_bin_center\"\n",
    "        #agg[bin_center_col] = [iv.mid for iv in agg.index]\n",
    "        #agg[cast_col] = cid\n",
    "        #agg[\"phase\"] = ph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5d0a6d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=gg.groupby(\"_bin\", observed=True)['rois_count'].sum(min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fdff8c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fix_latitude': 'mean',\n",
       " 'fix_longitude': 'mean',\n",
       " 'salinity': 'mean',\n",
       " 'temperature': 'mean',\n",
       " 'mass_concentration_of_oxygen_in_sea_water': 'mean',\n",
       " 'bin_mean_sea_water_salinity': 'mean',\n",
       " 'bin_median_sea_water_salinity': 'mean',\n",
       " 'bin_mean_sea_water_temperature': 'mean',\n",
       " 'bin_median_sea_water_temperature': 'mean',\n",
       " 'PAR': 'sum',\n",
       " 'chlorophyll': 'mean',\n",
       " 'chl': 'mean',\n",
       " 'bbp470': 'mean',\n",
       " 'bbp650': 'mean',\n",
       " 'volumescatcoeff117deg470nm': 'mean',\n",
       " 'volumescatcoeff117deg650nm': 'mean',\n",
       " 'particulatebackscatteringcoeff470nm': 'mean',\n",
       " 'particulatebackscatteringcoeff650nm': 'mean',\n",
       " 'fix_residual_percent_distance_traveled_DeadReckonUsingMultipleVelocitySources': 'sum',\n",
       " 'latitude': 'mean',\n",
       " 'longitude': 'mean',\n",
       " 'turning_point': 'mean',\n",
       " 'cast_id_unique': 'mean',\n",
       " 'distance_km': 'mean',\n",
       " 'rois_count': 'sum',\n",
       " 'rois_norm': 'sum',\n",
       " 'Akashiwo': 'sum',\n",
       " 'Amphidinium_Oxyphysis': 'sum',\n",
       " 'Ceratium': 'sum',\n",
       " 'Chaetoceros': 'sum',\n",
       " 'Ciliate': 'sum',\n",
       " 'Cylindrotheca': 'sum',\n",
       " 'Detonula_Cerataulina_Lauderia': 'sum',\n",
       " 'Detritus': 'sum',\n",
       " 'Dictyocha': 'sum',\n",
       " 'Dinoflagellate': 'sum',\n",
       " 'Eucampia': 'sum',\n",
       " 'Guinardia_Dactyliosolen': 'sum',\n",
       " 'Gyrodinium': 'sum',\n",
       " 'Medium_pennate': 'sum',\n",
       " 'Mesodinium': 'sum',\n",
       " 'Mixed_diatom_chain': 'sum',\n",
       " 'Nano_plankton': 'sum',\n",
       " 'Polykrikos': 'sum',\n",
       " 'Prorocentrum': 'sum',\n",
       " 'Pseudo-nitzschia': 'sum',\n",
       " 'Strombidium': 'sum',\n",
       " 'Thalassionema': 'sum',\n",
       " 'Thalassiosira': 'sum',\n",
       " 'Tiarina': 'sum',\n",
       " 'Truncated': 'sum',\n",
       " 'unknown_flagellate': 'sum'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51c62072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xarray as xr\n",
    "#xf=xr.open_dataset(p / \"Ahi_20250414_20250418.nc\")\n",
    "#dlr=xf.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa6e9fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned = bin_casts_avg_with_time_updown(\n",
    "    df.reset_index(drop=False),\n",
    "    depth_col=\"depth\",\n",
    "    cast_col=\"cast_id\",\n",
    "    time_col=\"time\",   # or None to use datetime index\n",
    "    bin_size=5,\n",
    "    phase_labels=(\"down\", \"up\"),\n",
    "    smooth_window=5,\n",
    "#    smooth_window=17,\n",
    "    closed=\"right\",\n",
    "    per_phase_edges=True,  # True if you want separate edges for up/down\n",
    ")\n",
    "binned = binned.reset_index()\n",
    "binned['sigma_theta'] = gsw.sigma0(binned['salinity'], binned['temperature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93428842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "15429     5\n",
       "15430    52\n",
       "15431    74\n",
       "15432    61\n",
       "15433    12\n",
       "Name: rois_count, Length: 15434, dtype: Int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binned['rois_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d2833",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned.to_csv(p / \"lrauv_april2025_binned_depthtime_updown_5m_perphase_edges.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7249080",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab979096",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1,ax1 = plt.subplots(figsize=(12, 3))\n",
    "cax1 = ax1.scatter(binned['start_time'], binned['depth_bin_center'],c=binned['Pseudo-nitzschia'],clim=(0,2),s=5)\n",
    "plt.colorbar(cax1, ax=ax1, label='PN Image Count (images/s/2 meter)')\n",
    "ax1.invert_yaxis()\n",
    "#ax1.invert_xaxis()\n",
    "ax1.set_ylabel('Depth (m)')\n",
    "ax1.set_xlim(dt.datetime(2025,4,16,9,40),dt.datetime(2025,4,14,18,0,0))\n",
    "#plt.colorbar(cax1, ax=ax1, label='PN Image Count (images/s/2 meter)')\n",
    "#plt.savefig(fig_dir / 'April2025_PN_depth_binned_outbound.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "#ax1.set_xlim(dt.datetime(2025,4,14,18,0,0),dt.datetime(2025,4,16,9,40))\n",
    "\n",
    "fig2,ax2 = plt.subplots(figsize=(12, 3))\n",
    "cax2 = ax2.scatter(binned['start_time'], binned['depth_bin_center'],c=binned['Pseudo-nitzschia'],clim=(0,2),s=5)\n",
    "plt.colorbar(cax2, ax=ax2, label='PN Image Count (images/s/2 meter)')\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_ylabel('Depth (m)')\n",
    "ax2.set_xlim(dt.datetime(2025,4,16,9,40),dt.datetime(2025,4,19))\n",
    "#plt.savefig(fig_dir / 'April2025_PN_depth_binned_inbound.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b36a6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a86da",
   "metadata": {},
   "outputs": [],
   "source": [
    "turn = dt.datetime(2025,4,16,9,40)\n",
    "out_trans = binned.query(\"start_time < @turn\")\n",
    "in_trans = binned.query(\"start_time >= @turn\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "#label = \"Detritus\"\n",
    "label = \"Pseudo-nitzschia\"\n",
    "#label = \"Chaetoceros\"\n",
    "#label = \"Ciliate\"\n",
    "#label = \"Medium_pennate\"\n",
    "#sc=ax.pcolor(out_trans.distance_km,out_trans['depth'],out_trans[label])\n",
    "sc = ax.scatter(\n",
    "    out_trans.distance_km,out_trans['depth_bin_center'], c=out_trans[label])\n",
    "    #out_trans.distance_km,out_trans['depth'], c=out_trans[label])\n",
    "sc.set_clim(0, 2)\n",
    "ax.invert_yaxis()\n",
    "ax.invert_xaxis()\n",
    "ax.set_ylabel('Depth (m)')\n",
    "ax.set_xlabel('Distance (km)')\n",
    "plt.colorbar(sc, ax=ax, label='PN Image Count (images/s/2 meter)')\n",
    "\n",
    "#plt.colorbar(sc, label=label)\n",
    "# plt.title('LRAUV Optical Backscatter (470nm) April 2025')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(fig_dir / 'April2025_MP_outbound_distance.png', dpi=300, bbox_inches='tight')    \n",
    "plt.savefig(fig_dir / 'April2025_PN_outbound_distance.png', dpi=300, bbox_inches='tight')\n",
    "#plt.savefig(fig_dir / 'April2025_detritus_outbound_distance.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "sc = ax.scatter(\n",
    "    in_trans.distance_km,in_trans['depth_bin_center'], c=in_trans[label])\n",
    "    #in_trans.distance_km,in_trans['depth'], c=in_trans[label])\n",
    "sc.set_clim(0, 2)\n",
    "ax.invert_yaxis()\n",
    "ax.invert_xaxis()\n",
    "ax.set_ylabel('Depth (m)')\n",
    "ax.set_xlabel('Distance (km)')\n",
    "plt.colorbar(sc, ax=ax, label='PN Image Count (images/s/2 meter)')\n",
    "\n",
    "#plt.colorbar(sc, label=label)\n",
    "# plt.title('LRAUV Optical Backscatter (470nm) April 2025')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(fig_dir / 'April2025_MP_inbound_distance.png', dpi=300, bbox_inches='tight')    \n",
    "plt.savefig(fig_dir / 'April2025_PN_inbound_distance.png', dpi=300, bbox_inches='tight')    \n",
    "#plt.savefig(fig_dir / 'April2025_detritus_inbound_distance.png', dpi=300, bbox_inches='tight')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b60cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac41c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(out_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1742ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_trans.to_csv('April2025_outbound_transect_PO_and_classified_sum_withNaN.csv')\n",
    "#out_trans.to_parquet('April2025_outbound_bined_labeled.parquet')\n",
    "#binned.to_parquet('April2025_bined_and_labeled_model20251009_filtered_window5_sumvmean_withNaN.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce7d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_trans['rois_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf03cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid=np.unique(out_trans['cast_id'])\n",
    "#lp=np.arange(0,5)\n",
    "bins_d=out_trans['depth_bin_center'].values\n",
    "lp=np.arange(0,len(uid))\n",
    "#print(bins_d.max(),bins_d.min())\n",
    "phase=out_trans['phase'].values\n",
    "castid=out_trans['cast_id'].values\n",
    "label='Pseudo-nitzschia'\n",
    "#label='Detritus'\n",
    "species=out_trans[label].values\n",
    "distance=out_trans['distance_km'].values\n",
    "dvec=np.arange(2.5,203.5,5)\n",
    "# number of profiles up and down\n",
    "nsp=np.zeros((len(dvec),len(2*lp)))\n",
    "nsp=nsp*np.nan\n",
    "ndp=np.zeros((len(dvec),len(lp)))\n",
    "ndp=ndp*np.nan\n",
    "ndd=np.zeros((len(dvec),len(lp)))\n",
    "ndd=ndd*np.nan\n",
    "for i in lp:\n",
    "    index=np.where((castid==uid[i]) & (phase=='down'))\n",
    "    bd=bins_d[index]\n",
    "    sp=species[index]\n",
    "    dist=distance[index]\n",
    "    for j in range(0,len(dvec)-1):\n",
    "        ndp[j,i]=dvec[j]\n",
    "        ind=np.where((bd>=dvec[j]) & (bd<dvec[j+1]))\n",
    "        if len(ind[0])>0:\n",
    "            nsp[j,i]=np.nanmean(sp[ind])\n",
    "            ndd[j,i]=np.nanmean(dist[ind])\n",
    "    index=np.where((castid==uid[i]) & (phase=='up'))\n",
    "    bd=bins_d[index]\n",
    "    sp=species[index]\n",
    "    dist=distance[index]\n",
    "    for j in range(0,len(dvec)-1):\n",
    "        ndp[j,i]=dvec[j]\n",
    "        ind=np.where((bd>=dvec[j]) & (bd<dvec[j+1]))\n",
    "        if len(ind[0])>0:\n",
    "            nsp[j,i]=np.nanmean(sp[ind])\n",
    "            ndd[j,i]=np.nanmean(dist[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid=np.unique(in_trans['cast_id'])\n",
    "#lp=np.arange(0,5)\n",
    "bins_d=in_trans['depth_bin_center'].values\n",
    "lp=np.arange(0,len(uid))\n",
    "#print(bins_d.max(),bins_d.min())\n",
    "phase=in_trans['phase'].values\n",
    "castid=in_trans['cast_id'].values\n",
    "#label='Pseudo-nitzschia'\n",
    "label='Detritus'\n",
    "species=in_trans[label].values\n",
    "distance=in_trans['distance_km'].values\n",
    "dvec=np.arange(2.5,203.5,5)\n",
    "# number of profiles up and down\n",
    "nsp=np.zeros((len(dvec),len(2*lp)))\n",
    "nsp=nsp*np.nan\n",
    "ndp=np.zeros((len(dvec),len(lp)))\n",
    "ndp=ndp*np.nan\n",
    "ndd=np.zeros((len(dvec),len(lp)))\n",
    "ndd=ndd*np.nan\n",
    "for i in lp:\n",
    "    index=np.where((castid==uid[i]) & (phase=='down'))\n",
    "    bd=bins_d[index]\n",
    "    sp=species[index]\n",
    "    dist=distance[index]\n",
    "    for j in range(0,len(dvec)-1):\n",
    "        ndp[j,i]=dvec[j]\n",
    "        ind=np.where((bd>=dvec[j]) & (bd<dvec[j+1]))\n",
    "        if len(ind[0])>0:\n",
    "            nsp[j,i]=np.nanmean(sp[ind])\n",
    "            ndd[j,i]=np.nanmean(dist[ind])\n",
    "    index=np.where((castid==uid[i]) & (phase=='up'))\n",
    "    bd=bins_d[index]\n",
    "    sp=species[index]\n",
    "    dist=distance[index]\n",
    "    for j in range(0,len(dvec)-1):\n",
    "        ndp[j,i]=dvec[j]\n",
    "        ind=np.where((bd>=dvec[j]) & (bd<dvec[j+1]))\n",
    "        if len(ind[0])>0:\n",
    "            nsp[j,i]=np.nanmean(sp[ind])\n",
    "            ndd[j,i]=np.nanmean(dist[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d4509",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "plot=ax.pcolor(ndd,-ndp,nsp,cmap='plasma',vmin=0,vmax=4)\n",
    "#ax.invert_yaxis()\n",
    "ax.set_xlabel('Distance (km)')\n",
    "ax.set_ylabel('Depth (m)')\n",
    "#plot.set_array(nsp.flatten())\n",
    "fig.colorbar(plot,ax=ax)\n",
    "#plt.tight_layout() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b96024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import dates as mdates\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# --- 0) Your input long-form data ---\n",
    "# binned columns needed: start_time (datetime-like), depth (m), sigma_theta (kg/m^3)\n",
    "binned_df = binned.dropna(subset=[\"start_time\", \"depth_bin_center\", \"sigma_theta\"]).copy()\n",
    "binned_df[\"start_time\"] = pd.to_datetime(binned_df[\"start_time\"])\n",
    "\n",
    "# --- 1) Build a regular target grid (you pick the resolution) ---\n",
    "# time grid: from min to max with, e.g., 2-minute spacing (adjust as needed)\n",
    "tmin, tmax = binned_df[\"start_time\"].min(), binned_df[\"start_time\"].max()\n",
    "time_grid = pd.date_range(tmin, tmax, freq=\"60min\")  # change interval as appropriate\n",
    "\n",
    "# depth grid: e.g., 0.5 m spacing from shallowest to deepest observed\n",
    "zmin, zmax = binned_df[\"depth_bin_center\"].min(), binned_df[\"depth_bin_center\"].max()\n",
    "depth_grid = np.arange(np.floor(zmin), np.ceil(zmax) + 1, 10)\n",
    "\n",
    "# Convert times to Matplotlib date numbers (floats) for contouring\n",
    "t_num = mdates.date2num(binned_df[\"start_time\"].to_numpy())\n",
    "Tg_num = mdates.date2num(time_grid.to_pydatetime())\n",
    "\n",
    "\n",
    "# --- 2) Interpolate onto the grid ---\n",
    "# Prepare source points and values\n",
    "points = np.column_stack([t_num, binned_df[\"depth_bin_center\"].to_numpy()])\n",
    "values = binned_df[\"sigma_theta\"].to_numpy()\n",
    "\n",
    "# Target mesh\n",
    "TTg, ZZg = np.meshgrid(Tg_num, depth_grid)\n",
    "\n",
    "# Interpolate; 'linear' is typical; fall back to 'nearest' for gaps\n",
    "SS = griddata(points, values, (TTg, ZZg), method=\"linear\")\n",
    "# SS   = griddata(points, values, (TTg, ZZg), method=\"nearest\")\n",
    "\n",
    "# Fill holes left by linear interpolation with nearest-neighbor\n",
    "# SS = np.where(np.isnan(SS_linear), SS_near, SS_linear)\n",
    "\n",
    "# Optional: mask impossible ranges or leave NaN to avoid misleading contours\n",
    "# SS[(SS < 10) | (SS > 40)] = np.nan  # example sanity check\n",
    "\n",
    "# --- 3) Choose contour levels ---\n",
    "# You can pick a fixed range or use percentiles to adapt to each dataset\n",
    "valid = SS[np.isfinite(SS)]\n",
    "vmin, vmax = np.percentile(valid, [5, 95]) if valid.size else (24, 27)\n",
    "levels = np.arange(np.floor(vmin*10)/10, np.ceil(vmax*10)/10 + 1e-9, 0.2)  # every 0.2 kg/m^3\n",
    "levels = [25.8,26,26.2]\n",
    "\n",
    "# --- 4) Plot: base scatter (optional) + isopycnal contours ---\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# # (Optional) show original points\n",
    "sc = ax.scatter(binned_df[\"start_time\"], binned_df[\"depth_bin_center\"], c=binned_df[\"Pseudo-nitzschia\"], s=10, cmap=\"viridis\",vmin=0,vmax=2)\n",
    "plt.colorbar(sc, ax=ax, label=\"Normalized Image Count\")\n",
    "\n",
    "# Contours (isopycnals) from gridded field\n",
    "CS = ax.contour(mdates.num2date(TTg), ZZg, SS, levels=levels, colors=\"w\", linewidths=2)\n",
    "ax.clabel(CS, inline=True, fontsize=8, fmt=\"%.1f\")  # label lines\n",
    "\n",
    "# Format axes\n",
    "ax.invert_yaxis()\n",
    "ax.set_ylabel(\"Depth (m)\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\\n%H:%M\"))\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41b05e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
